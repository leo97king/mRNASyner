{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##以下均为数据处理流程\n",
    "f1=open('D:/unilm/data/hg38_mrna.fasta','r')\n",
    "f2=open('D:/unilm/data/mrna.txt','w')\n",
    "#a=f1.readlines()\n",
    "s=0\n",
    "for lines in f1.readlines():\n",
    "    if '>' in lines:\n",
    "        s=s+1\n",
    "        if s == 1:\n",
    "            continue\n",
    "        else:\n",
    "            f2.write('\\n')\n",
    "            continue\n",
    "    f2.write(lines.strip('\\n'))\n",
    "f1.close()\n",
    "f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#对输入的fasta格式文件筛选，选取<1500长度的UTR且<6000长度的CDS序列作为数据集\n",
    "f2=open('D:/unilm/data/mrna.txt','r')\n",
    "f3=open('D:/unilm/data/mrna_final.txt','w')\n",
    "for lines in f2.readlines():\n",
    "    list_5utr=[]\n",
    "    list_cds=[]\n",
    "    list_3utr=[]\n",
    "    y=0\n",
    "    for strings in lines:\n",
    "        if strings.isupper():\n",
    "            list_cds.append(strings)\n",
    "            y=y+1\n",
    "            continue\n",
    "        if y != 0 :\n",
    "            if strings.islower():\n",
    "                list_3utr.append(strings)\n",
    "                continue\n",
    "        list_5utr.append(strings)                   \n",
    "    if len(list_5utr) <= 1500 and len(list_5utr) >1 and len(list_cds) <= 6000 and len(list_cds)!=0 and len(list_3utr) <= 1500 and len(list_3utr) != 0 :\n",
    "        f3.write(lines)\n",
    "f2.close()\n",
    "f3.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#检查筛选后序列数量\n",
    "f3=open('D:/unilm/data/mrna_final.txt','r')\n",
    "a=0\n",
    "b=0\n",
    "c=0\n",
    "for lines in f3.readlines():\n",
    "    list_5utr=[]\n",
    "    list_cds=[]\n",
    "    list_3utr=[]\n",
    "    y=0\n",
    "    for strings in lines:\n",
    "        if strings.isupper():\n",
    "            list_cds.append(strings)\n",
    "            y=y+1\n",
    "            continue\n",
    "        if y != 0 :\n",
    "            if strings.islower():\n",
    "                list_3utr.append(strings)\n",
    "                continue\n",
    "        list_5utr.append(strings)            \n",
    "    if len(list_5utr) <= 20 :\n",
    "        a=a+1\n",
    "    if len(list_cds) <= 6000 and len(list_cds) !=0 :\n",
    "        b=b+1\n",
    "    if len(list_3utr) <= 50 :\n",
    "        c=c+1\n",
    "print(a,b,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#按照8：1：1划分数据集\n",
    "import numpy as np\n",
    "data=open('D:/unilm/data/mrna_final.txt','r').readlines()\n",
    "np.random.shuffle(data)\n",
    "train_size=int(0.8*len(data))\n",
    "valid_size=int(0.1*len(data))\n",
    "train_data=data[:train_size]\n",
    "valid_data=data[train_size:train_size+valid_size]\n",
    "test_data =data[train_size+valid_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#生成训练集、验证集和测试集\n",
    "f4=open('D:/unilm/data/train.txt','w')\n",
    "f5=open('D:/unilm/data/valid.txt','w')\n",
    "f6=open('D:/unilm/data/test.txt','w')\n",
    "for lines in train_data:\n",
    "    f4.write(lines)\n",
    "for lines in valid_data:\n",
    "    f5.write(lines)\n",
    "for lines in test_data:\n",
    "    f6.write(lines)\n",
    "f4.close()\n",
    "f5.close()\n",
    "f6.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#分别生成CDS，5utr，3utr的标准输入格式，CDS为真值，utr为预测值，后续进行机器翻译任务，CDS为src_file,5utr和3utr为tgt_file\n",
    "f3=open('D:/unilm/data/valid.txt','r')\n",
    "f4=open('D:/unilm/data/val_cds.txt','w')\n",
    "f5=open('D:/unilm/data/val_3utr.txt','w')\n",
    "f6=open('D:/unilm/data/val_5utr.txt','w')\n",
    "for lines in f3.readlines():\n",
    "    lines= lines.strip(' ')\n",
    "    list_5utr=[]\n",
    "    list_cds=[]\n",
    "    list_3utr=[]\n",
    "    y=0\n",
    "    str_5utr=''\n",
    "    str_3utr=''\n",
    "    for strings in lines:\n",
    "        if strings.isupper():\n",
    "            list_cds.append(strings)\n",
    "            y=y+1\n",
    "            continue\n",
    "        if y != 0 :\n",
    "            if strings.islower():\n",
    "                list_3utr.append(strings)\n",
    "                continue\n",
    "        list_5utr.append(strings)\n",
    "    f4.write(''.join(list_cds).upper()+'\\n')\n",
    "    f5.write(''.join(list_3utr).upper()+'\\n')\n",
    "    f6.write(''.join(list_5utr).upper())\n",
    "f3.close()\n",
    "f4.close()\n",
    "f5.close()\n",
    "f6.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##以下均为K-mers词典构建流程（根据K值的不同需要改读取和保存的文件名）\n",
    "#将txt格式转换为fasta\n",
    "f1=open('D:/unilm/data/3utr.txt','r')\n",
    "f2=open('D:/unilm/data/3utr.fasta','w')\n",
    "for lines in f1.readlines():\n",
    "    f2.write('>'+'\\n')\n",
    "    f2.write(lines)\n",
    "f1.close()\n",
    "f2.close()\n",
    "\n",
    "#计算K-mers,输出所有子片段的频率\n",
    "from Bio import SeqIO\n",
    "def build_kmers(seq, k_size):\n",
    "    \"\"\"a function to calculate kmers from seq\"\"\"\n",
    "    kmers = []  # k-mer存储在列表中\n",
    "    n_kmers = len(seq) - k_size + 1\n",
    "    \n",
    "    for i in range(n_kmers):\n",
    "        kmer = seq[i:i + k_size]\n",
    "        kmers.append(kmer)\n",
    "        \n",
    "    return kmers\n",
    "\n",
    "from collections import Counter\n",
    "def summary_kmers(kmers):\n",
    "    \"\"\"a function to summarize the kmers\"\"\"\n",
    "    kmers_stat = Counter(kmers)\n",
    "    return kmers_stat\n",
    "sum=Counter()\n",
    "for seq_record in SeqIO.parse('3utr.fasta','fasta'):\n",
    "    a=summary_kmers(build_kmers(seq_record.seq.upper(),3))\n",
    "    sum=sum+a\n",
    "print(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_3mers=dict(sum)\n",
    "list1=[]\n",
    "list2=[]\n",
    "mersdict={}\n",
    "for keys in _3mers.keys():\n",
    "    list1.append(keys)\n",
    "    list2.append(_3mers[keys])\n",
    "mersdict['seq']=list1\n",
    "mersdict['num']=list2\n",
    "import pandas as pd\n",
    "df=pd.DataFrame(mersdict)\n",
    "df.to_csv('D:/unilm/data/3mers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('D:/unilm/data/k-mers/5utr/8mers.csv')\n",
    "df_sort=df.sort_values(by=['num'],ascending=[False])\n",
    "b=df_sort.reset_index(drop=True)\n",
    "a=b.seq.iloc[:65536]\n",
    "result=a.to_list()\n",
    "f1=open('D:/unilm/data/k-mers/3utr/8mers_all.txt','w')\n",
    "for i in result:\n",
    "    f1.write(str(i)+'\\n')\n",
    "f1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##以下均为Rouge评价流程\n",
    "from rouge import FilesRouge\n",
    "from rouge import Rouge\n",
    "def rouge_score(pre_list,true_list):\n",
    "    rouge = Rouge()\n",
    "    for i in range(len(pre_list)):\n",
    "        if pre_list[i]=='' or pre_list==' ':\n",
    "            pre_list[i]='#'\n",
    "    rouge_scores = rouge.get_scores(pre_list, true_list,avg=True)\n",
    "    return rouge_scores\n",
    "pre_list=open('D:/bert_seq2seq/result/bert_2048_5utr.txt','r').readlines()\n",
    "true_list=open('D:/bert_seq2seq/data/test/test_5utr_token_new.txt','r').readlines()\n",
    "rouge_score(pre_list,true_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=scores['rouge-1']\n",
    "b=(a['r']+a['p']+a['f'])/3\n",
    "b=(a['r']+a['p']+a['f'])/3\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##以下均为Jaccard评价流程\n",
    "def jaccard_similarity(set1, set2):\n",
    "    # 计算两个集合的交集\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    # 计算两个集合的并集\n",
    "    union = len(set1.union(set2))\n",
    "    # 计算Jaccard系数\n",
    "    jaccard_coefficient = intersection / union\n",
    "    return jaccard_coefficient\n",
    "\n",
    "def load_text_file(file):\n",
    "        # 将文本拆分为单词，并创建一个包含单词的集合\n",
    "        word_set=(set(file.split()))\n",
    "        return word_set\n",
    "f1=open(\"D:/bert_seq2seq/result/bert_2048_5utr.txt\",'r')\n",
    "f2=open(\"D:/bert_seq2seq/data/test/test_5utr_token_new.txt\",'r')\n",
    "document1=f1.readlines()\n",
    "document2=f2.readlines()\n",
    "jaccard_list=[]\n",
    "for i in range(len(document1)):\n",
    "    jaccard_coefficient = jaccard_similarity(load_text_file(document1[i]),load_text_file(document2[i]))\n",
    "    jaccard_list.append(jaccard_coefficient)\n",
    "\n",
    "print(\"Jaccard系数:\", sum(jaccard_list)/len(jaccard_list))\n",
    "f1.close()\n",
    "f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##以下均为BLEU评价流程\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "f1=open(\"D:/bert_seq2seq/result/bert_2048_3utr.txt\",'r')\n",
    "f2=open(\"D:/bert_seq2seq/data/test/test_3utr_token_new.txt\",'r')\n",
    "pre_list=f1.readlines()\n",
    "true_list=f2.readlines()\n",
    "def bleu_score(pre_list,true_list):\n",
    "    candidates=[]\n",
    "    references=[]\n",
    "    for i in range(len(pre_list)):\n",
    "        if pre_list[i]=='' or pre_list==' ':\n",
    "            pre_list[i]='#'\n",
    "    for x in pre_list:\n",
    "        x=x.strip('\\n').strip(' ')\n",
    "        candidates.append(list(x.split(' ')))\n",
    "    for y in true_list:\n",
    "        y=y.strip('\\n').strip(' ')        \n",
    "        references.append(list(y.split(' ')))\n",
    "    print(candidates[0],references[0])\n",
    "    return corpus_bleu(references,candidates) \n",
    "bleu_score(pre_list,true_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#k-mers分析（寻找特殊核苷酸片段以供优化参考）（比较原始训练集和生成后排名前10的序列，验证模型的有效性）\n",
    "codon_file=open('D:/bert_seq2seq/result/bert_2048_5utr.txt','r')\n",
    "codon_file=codon_file.readlines()\n",
    "def build_kmers(seq, k_size):\n",
    "    kmers = []  # k-mer存储在列表中\n",
    "    n_kmers = len(seq) - k_size + 1\n",
    "    \n",
    "    for i in range(n_kmers):\n",
    "        kmer = seq[i:i + k_size]\n",
    "        kmers.append(kmer)\n",
    "        \n",
    "    return kmers\n",
    "from collections import Counter\n",
    "def summary_kmers(kmers):\n",
    "    kmers_stat = Counter(kmers)\n",
    "    return kmers_stat\n",
    "kmers_dic={}\n",
    "for k in range(3,9):\n",
    "    sum=Counter()\n",
    "    for seq in codon_file:\n",
    "        seq=seq.strip('\\n').replace(' ','')\n",
    "        a=summary_kmers(build_kmers(seq,k))\n",
    "        sum=sum+a\n",
    "    kmers_dic['{}'.format(k)]=sum\n",
    "    \n",
    "print(kmers_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#对得到的k-mers进行分别排序，并取出前100放入dataframe\n",
    "import pandas as pd\n",
    "df_kmers=pd.DataFrame(columns=['5','6','7','8'])\n",
    "for k in range(5,9):\n",
    "    list1=[]\n",
    "    list2=[]\n",
    "    mersdict={}\n",
    "    for keys in kmers_dic['{}'.format(k)]:\n",
    "        list1.append(keys)      #片段\n",
    "        list2.append(kmers_dic['{}'.format(k)][keys])    #数量\n",
    "    mersdict['seq']=list1\n",
    "    mersdict['num']=list2\n",
    "    df_tmp=pd.DataFrame(mersdict)\n",
    "    df_tmp=df_tmp.sort_values(by=['num'],ascending=False)\n",
    "    df_tmp=df_tmp.reset_index(drop=True)\n",
    "    df_kmers['{}'.format(k)]=df_tmp['seq'][:100]\n",
    "print(df_kmers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算每个k-mers的位置概率，并输出概率矩阵，用以制作motif图\n",
    "kmers=pd.read_csv('D:/bert_crf-master/kmers_3utr_motif.csv')\n",
    "print(kmers)\n",
    "for k in range(5,9):\n",
    "    k_list=[]\n",
    "    for pos in range(k):\n",
    "        pos_list=[]\n",
    "        pos_dic={}\n",
    "        for rank in range(100):\n",
    "            pos_list.append(kmers['{}'.format(k)][rank][pos])\n",
    "        pos_count=Counter(pos_list)\n",
    "        pos_dic['G']=pos_count['G']/(pos_count['G']+pos_count['C']+pos_count['A']+pos_count['U'])\n",
    "        pos_dic['C']=pos_count['C']/(pos_count['G']+pos_count['C']+pos_count['A']+pos_count['U'])\n",
    "        pos_dic['A']=pos_count['A']/(pos_count['G']+pos_count['C']+pos_count['A']+pos_count['U'])\n",
    "        pos_dic['U']=pos_count['U']/(pos_count['G']+pos_count['C']+pos_count['A']+pos_count['U'])\n",
    "        #print(pos_dic)\n",
    "        k_list.append(pos_dic)\n",
    "    print(k_list)\n",
    "    k_df=pd.DataFrame(k_list).T\n",
    "    k_df.to_csv('{}mers_3utr.csv'.format(k))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
